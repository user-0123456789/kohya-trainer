{"cells":[{"cell_type":"markdown","source":["This colab requires you to have model.ckpt on your google drive (or you can download it on the next step)"],"metadata":{"collapsed":false,"id":"f20_rzsJc1zU"}},{"cell_type":"code","source":["#@title Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RJeVH0KtdVBh","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"outputId":"34e7ed19-71b6-4a0b-e80b-c8db1e3221e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"JtgQHPFgc1zZ","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1686335287757,"user_tz":180,"elapsed":127236,"user":{"displayName":"fabio alcantara","userId":"18319269270041367663"}},"outputId":"03bd1d4d-e4e0-4c99-d06e-30ff0512314e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stable-diffusion'...\n","remote: Enumerating objects: 1658, done.\u001b[K\n","remote: Total 1658 (delta 0), reused 0 (delta 0), pack-reused 1658\u001b[K\n","Receiving objects: 100% (1658/1658), 150.16 MiB | 18.22 MiB/s, done.\n","Resolving deltas: 100% (918/918), done.\n","/content/stable-diffusion\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gradio\n","  Downloading gradio-3.34.0-py3-none-any.whl (20.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Collecting diffusers\n","  Downloading diffusers-0.17.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Collecting pudb\n","  Downloading pudb-2022.1.3.tar.gz (220 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting invisible-watermark\n","  Downloading invisible_watermark-0.1.5-py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.25.1)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.8)\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.0.3-py3-none-any.whl (720 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.6/720.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting test-tube\n","  Downloading test_tube-0.7.5.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting streamlit\n","  Downloading streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-fidelity\n","  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n","Collecting transformers\n","  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kornia\n","  Downloading kornia-0.6.12-py2.py3-none-any.whl (653 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.4/653.4 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yapf\n","  Downloading yapf-0.33.0-py2.py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tb-nightly\n","  Downloading tb_nightly-2.14.0a20230609-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles (from gradio)\n","  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n","Collecting aiohttp (from gradio)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.96.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client>=0.2.6 (from gradio)\n","  Downloading gradio_client-0.2.6-py3-none-any.whl (288 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio)\n","  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n","Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting mdit-py-plugins<=0.3.3 (from gradio)\n","  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n","Collecting orjson (from gradio)\n","  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.7)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n","Collecting python-multipart (from gradio)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n","Collecting semantic-version (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.0 (from gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.10.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.7.0.72)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.0)\n","Collecting importlib-metadata (from diffusers)\n","  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2022.10.31)\n","Collecting urwid>=1.1.1 (from pudb)\n","  Downloading urwid-2.1.2.tar.gz (634 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.6/634.6 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jedi<1,>=0.18 (from pudb)\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urwid_readline (from pudb)\n","  Downloading urwid_readline-0.13.tar.gz (7.9 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pudb) (23.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (2.0.1+cu118)\n","Collecting onnx (from invisible-watermark)\n","  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime (from invisible-watermark)\n","  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark) (1.4.1)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n","Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from test-tube) (2.12.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from test-tube) (0.18.3)\n","Collecting blinker<2,>=1.0.0 (from streamlit)\n","  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.3)\n","Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n","Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Collecting pympler<2,>=0.9 (from streamlit)\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n","Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.3.4)\n","Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.2)\n","Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.3)\n","Collecting validators<1,>=0.2 (from streamlit)\n","  Downloading validators-0.20.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gitpython!=3.1.19,<4,>=3 (from streamlit)\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit)\n","  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.1)\n","Collecting watchdog (from streamlit)\n","  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m541.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (0.15.2+cu118)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (2.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (1.54.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (3.4.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (0.7.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tb-nightly) (0.40.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->gradio)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->gradio)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->gradio)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->gradio)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->gradio)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.15.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi<1,>=0.18->pudb) (0.8.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n","Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n","  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (3.4)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube) (1.8.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->invisible-watermark) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->invisible-watermark) (16.0.5)\n","Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal<5,>=1.1->streamlit) (0.1.0.post0)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators<1,>=0.2->streamlit) (4.4.2)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n","  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.0.9)\n","Collecting coloredlogs (from onnxruntime->invisible-watermark)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->invisible-watermark) (23.3.3)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n","Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n","  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly) (3.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->invisible-watermark)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit) (2023.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible-watermark) (1.3.0)\n","Building wheels for collected packages: pudb, antlr4-python3-runtime, test-tube, urwid, validators, ffmpy, urwid_readline\n","  Building wheel for pudb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pudb: filename=pudb-2022.1.3-py3-none-any.whl size=70221 sha256=e846fcd1c80e66990ab1621950184bf24970eed8f3b4f93c24daa4e985c0e317\n","  Stored in directory: /root/.cache/pip/wheels/31/c3/a3/d574377e0c9c7a8a032ec19e21e5ec8df0590af0cda2de1315\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=bc03a9e513dd90882b1b724a1fccf90af643231547359d5a1f87d845c5950143\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25329 sha256=d6cd7115c6dd87f07d81bec556191439197ac2c3fcbe2a7b9621ac69b3051ccb\n","  Stored in directory: /root/.cache/pip/wheels/28/d4/8b/1aeb47c0dedd931b8e6aec55a8091864a69ac6f0adc5b12ea9\n","  Building wheel for urwid (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for urwid: filename=urwid-2.1.2-cp310-cp310-linux_x86_64.whl size=259902 sha256=a546594db3c199a25e3721dbf54dad2d51afdbe47ac45013ec6497031ae88676\n","  Stored in directory: /root/.cache/pip/wheels/05/3d/85/cde786c07f333509d868e5024d5ed8c70519fa1b8e8c66ec6c\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=34bad0fd95bce697046895b0efbb72c8750b98a9d59baedaec7677426ee8c4f8\n","  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=5e74252a816cf6643282555d4148207b78f7ead869600b7462c54a1a40cd3463\n","  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n","  Building wheel for urwid_readline (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for urwid_readline: filename=urwid_readline-0.13-py3-none-any.whl size=7550 sha256=d32336e67826b78d602cc11ae5ab1fce3025a21cb54ddb044dd771c5041aa169\n","  Stored in directory: /root/.cache/pip/wheels/58/1d/d8/20c6d76afd5bd205f5f95f19640df9a4e88fc6f1a4c25bb693\n","Successfully built pudb antlr4-python3-runtime test-tube urwid validators ffmpy urwid_readline\n","Installing collected packages: urwid, tokenizers, safetensors, pydub, ffmpy, antlr4-python3-runtime, yapf, websockets, watchdog, validators, urwid_readline, uc-micro-py, smmap, semantic-version, python-multipart, pympler, orjson, onnx, omegaconf, multidict, lightning-utilities, jedi, importlib-metadata, humanfriendly, h11, frozenlist, einops, blinker, async-timeout, aiofiles, yarl, uvicorn, starlette, pydeck, pudb, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, gitdb, coloredlogs, aiosignal, transformers, onnxruntime, httpx, gitpython, fastapi, diffusers, aiohttp, tb-nightly, streamlit, gradio-client, gradio, torchmetrics, torch-fidelity, test-tube, pytorch-lightning, lpips, kornia, invisible-watermark\n","Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 blinker-1.6.2 coloredlogs-15.0.1 diffusers-0.17.0 einops-0.6.1 fastapi-0.96.0 ffmpy-0.3.0 frozenlist-1.3.3 gitdb-4.0.10 gitpython-3.1.31 gradio-3.34.0 gradio-client-0.2.6 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 humanfriendly-10.0 importlib-metadata-6.6.0 invisible-watermark-0.1.5 jedi-0.18.2 kornia-0.6.12 lightning-utilities-0.8.0 linkify-it-py-2.0.2 lpips-0.1.4 mdit-py-plugins-0.3.3 multidict-6.0.4 omegaconf-2.3.0 onnx-1.14.0 onnxruntime-1.15.0 orjson-3.9.1 pudb-2022.1.3 pydeck-0.8.1b0 pydub-0.25.1 pympler-1.0.1 python-multipart-0.0.6 pytorch-lightning-2.0.3 safetensors-0.3.1 semantic-version-2.10.0 smmap-5.0.0 starlette-0.27.0 streamlit-1.23.1 tb-nightly-2.14.0a20230609 test-tube-0.7.5 tokenizers-0.13.3 torch-fidelity-0.3.0 torchmetrics-0.11.4 transformers-4.30.1 uc-micro-py-1.0.2 urwid-2.1.2 urwid_readline-0.13 uvicorn-0.22.0 validators-0.20.0 watchdog-3.0.0 websockets-11.0.3 yapf-0.33.0 yarl-1.9.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n","  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /content/stable-diffusion/src/clip\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip) (0.15.2+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip) (0.2.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip) (16.0.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip) (1.3.0)\n","Installing collected packages: ftfy, clip\n","  Running setup.py develop for clip\n","Successfully installed clip-1.0 ftfy-6.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/crowsonkb/k-diffusion\n","  Cloning https://github.com/crowsonkb/k-diffusion to /tmp/pip-req-build-k_ibptxk\n","  Running command git clone --filter=blob:none --quiet https://github.com/crowsonkb/k-diffusion /tmp/pip-req-build-k_ibptxk\n","  Resolved https://github.com/crowsonkb/k-diffusion to commit c9fe758757e022f05ca5a53fa8fac28889e4f1cf\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate (from k-diffusion==0.0.15)\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting clean-fid (from k-diffusion==0.0.15)\n","  Downloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n","Collecting clip-anytorch (from k-diffusion==0.0.15)\n","  Downloading clip_anytorch-2.5.2-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (0.6.1)\n","Collecting jsonmerge (from k-diffusion==0.0.15)\n","  Downloading jsonmerge-1.9.0.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: kornia in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (0.6.12)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (8.4.0)\n","Collecting resize-right (from k-diffusion==0.0.15)\n","  Downloading resize_right-0.0.2-py3-none-any.whl (8.9 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (1.10.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (2.0.1+cu118)\n","Collecting torchdiffeq (from k-diffusion==0.0.15)\n","  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n","Collecting torchsde (from k-diffusion==0.0.15)\n","  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (0.15.2+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from k-diffusion==0.0.15) (4.65.0)\n","Collecting wandb (from k-diffusion==0.0.15)\n","  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->k-diffusion==0.0.15) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->k-diffusion==0.0.15) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->k-diffusion==0.0.15) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->k-diffusion==0.0.15) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->k-diffusion==0.0.15) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->k-diffusion==0.0.15) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->k-diffusion==0.0.15) (16.0.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from clean-fid->k-diffusion==0.0.15) (2.27.1)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip-anytorch->k-diffusion==0.0.15) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip-anytorch->k-diffusion==0.0.15) (2022.10.31)\n","Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonmerge->k-diffusion==0.0.15) (4.3.3)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion==0.0.15) (2.25.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion==0.0.15) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->k-diffusion==0.0.15) (1.4.1)\n","Collecting boltons>=20.2.1 (from torchsde->k-diffusion==0.0.15)\n","  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trampoline>=0.1.2 (from torchsde->k-diffusion==0.0.15)\n","  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion==0.0.15) (8.1.3)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion==0.0.15) (3.1.31)\n","Collecting sentry-sdk>=1.0.0 (from wandb->k-diffusion==0.0.15)\n","  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->k-diffusion==0.0.15)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb->k-diffusion==0.0.15)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb->k-diffusion==0.0.15)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion==0.0.15) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion==0.0.15) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->k-diffusion==0.0.15) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->k-diffusion==0.0.15) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->k-diffusion==0.0.15) (4.0.10)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion==0.0.15) (23.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion==0.0.15) (0.19.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid->k-diffusion==0.0.15) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid->k-diffusion==0.0.15) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid->k-diffusion==0.0.15) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid->k-diffusion==0.0.15) (3.4)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip-anytorch->k-diffusion==0.0.15) (0.2.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->k-diffusion==0.0.15) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->k-diffusion==0.0.15) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->k-diffusion==0.0.15) (5.0.0)\n","Building wheels for collected packages: k-diffusion, jsonmerge, pathtools\n","  Building wheel for k-diffusion (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for k-diffusion: filename=k_diffusion-0.0.15-py3-none-any.whl size=25646 sha256=c86c8a74f6204c82205b59f147b161b1a345c3ddedda09c2aea11af2fef65880\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rvg35hrs/wheels/f9/94/6c/ff76722fc481c7594161dba81149e2cb2a742246d13d8ea859\n","  Building wheel for jsonmerge (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonmerge: filename=jsonmerge-1.9.0-py3-none-any.whl size=18608 sha256=59e3cb9e94601b820d4060378c0419f128c1f41720e5e6ac3552f297ae2fbde3\n","  Stored in directory: /root/.cache/pip/wheels/45/30/22/18d39219b08402f26539c3e72a132353a31f49204ff35c8d8e\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=4927dfaf6d42afa9c6139b332ef3bd685c48badf02becb404795feef6a595e8b\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built k-diffusion jsonmerge pathtools\n","Installing collected packages: trampoline, resize-right, pathtools, boltons, setproctitle, sentry-sdk, docker-pycreds, jsonmerge, wandb, torchsde, torchdiffeq, clip-anytorch, clean-fid, accelerate, k-diffusion\n","Successfully installed accelerate-0.20.3 boltons-23.0.0 clean-fid-0.1.35 clip-anytorch-2.5.2 docker-pycreds-0.4.0 jsonmerge-1.9.0 k-diffusion-0.0.15 pathtools-0.1.2 resize-right-0.0.2 sentry-sdk-1.25.1 setproctitle-1.3.2 torchdiffeq-0.2.3 torchsde-0.2.5 trampoline-0.1.2 wandb-0.15.4\n"]}],"source":["#@title Install requirements\n","!git clone https://github.com/neonsecret/stable-diffusion.git\n","%cd /content/stable-diffusion\n","!pip install gradio albumentations diffusers opencv-python pudb invisible-watermark imageio imageio-ffmpeg pytorch-lightning omegaconf test-tube streamlit einops torch-fidelity transformers torchmetrics kornia lpips yapf tb-nightly\n","# !pip install taming\n","!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n","!pip install git+https://github.com/crowsonkb/k-diffusion"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["#@title Install requirements step 2\n","%cd /content/stable-diffusion\n","!pip install -e .\n","!pip install Pillow==8.4.0 taming-transformers-rom1504\n","!pip install --upgrade pytorch-lightning"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"ksvqjvync1zb","cellView":"form"}},{"cell_type":"code","source":["#@title # Installing xformers\n","%%capture\n","from IPython.display import HTML\n","from subprocess import getoutput\n","import os\n","\n","!pip install -e git+https://github.com/facebookresearch/xformers.git#egg=xformers\n","\n","if not os.path.exists('/sd/stable-diffusion-webui/xformers'):\n","  %cd /sd/stable-diffusion/src\n","  !git clone https://github.com/facebookresearch/xformers\n","  !cp -R '/sd/stable-diffusion/src/k-diffusion/k_diffusion' '/sd/stable-diffusion-webui/'\n","  !cp -R '/sd/stable-diffusion/src/xformers/xformers' '/sd/stable-diffusion-webui/'\n","  !cp -R '/sd/stable-diffusion/ldm' '/sd/stable-diffusion-webui/'\n","\n","%cd /content/\n","!git clone https://github.com/TheLastBen/fast-stable-diffusion\n","%cd /content/fast-stable-diffusion/precompiled\n","!mv /content/fast-stable-diffusion/precompiled/_C_flashattention.1 /content/fast-stable-diffusion/precompiled/_C_flashattention.7z.001\n","!mv /content/fast-stable-diffusion/precompiled/_C_flashattention.2 /content/fast-stable-diffusion/precompiled/_C_flashattention.7z.002\n","!7z x /content/fast-stable-diffusion/precompiled/_C_flashattention.7z.001\n","!mv -f /content/fast-stable-diffusion/precompiled/_C_flashattention.so /content/gdrive/MyDrive/sd/stable-diffusion-webui/xformers\n","!mv -f /content/fast-stable-diffusion/precompiled/_C.so /content/gdrive/MyDrive/sd/stable-diffusion-webui/xformers\n","\n","\n","%cd /sd/stable-diffusion-webui/ldm/modules"],"metadata":{"id":"a---cT2rwUQj","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Restarting the runtime here! Once it's done, just proceed to the next step.\n","import os\n","os.kill(os.getpid(), 9)"],"metadata":{"cellView":"form","id":"bFiD-_qJuNNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","#@title  Download a model { display-mode: \"form\" }\n","!pip install wget\n","Download_link = \"https://civitai.com/api/download/models/80511?type=Model&format=SafeTensor&size=pruned&fp=fp16\" #@param {type:\"string\"}\n","\n","To_folder = \"/content/\" #@param {type:\"string\"}\n","\n","Name = \"Meina.safetensors\"#@param {type:\"string\"}\n","\n","import wget\n","\n","!wget -O {Name} -P {To_folder} {Download_link}"],"metadata":{"id":"voKbhBMEe-yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Load the models\n","%cd /content/stable-diffusion\n","import gc\n","gc.collect()\n","import os\n","import sys\n","import git\n","if not os.path.exists(\"CodeFormer/\"):\n","    print(\"Installing CodeFormer..\")\n","    git.Repo.clone_from(\"https://github.com/sczhou/CodeFormer/\", \"CodeFormer\")\n","    os.chdir(\"CodeFormer\")\n","    os.system(\"python basicsr/setup.py develop\")\n","    os.chdir(\"..\")\n","    print(\"Installation successful\")\n","%cd /content/stable-diffusion\n","sys.path.append('CodeFormer/')\n","sys.path.append('../CodeFormer/')\n","sys.path.append('optimizedSD/')\n","\n","import argparse\n","import asyncio\n","import logging\n","import mimetypes\n","import re\n","import time\n","from contextlib import nullcontext\n","from itertools import islice\n","from random import randint\n","\n","import gradio as gr\n","import numpy as np\n","import torch\n","from PIL import Image\n","from einops import rearrange, repeat\n","from omegaconf import OmegaConf\n","from pytorch_lightning import seed_everything\n","from torch import autocast\n","from torchvision.utils import make_grid\n","from tqdm import tqdm, trange\n","from transformers import logging as transformers_logging\n","\n","from ldm.util import instantiate_from_config\n","from optimUtils import split_weighted_subprompts\n","\n","from basicsr.utils import img2tensor, tensor2img\n","from basicsr.utils.download_util import load_file_from_url\n","from facelib.utils.face_restoration_helper import FaceRestoreHelper\n","from basicsr.archs.rrdbnet_arch import RRDBNet\n","from basicsr.utils.realesrgan_utils import RealESRGANer\n","\n","from basicsr.utils.registry import ARCH_REGISTRY\n","from torchvision.transforms.functional import normalize\n","import cv2\n","\n","transformers_logging.set_verbosity_error()\n","\n","mimetypes.init()\n","mimetypes.add_type(\"application/javascript\", \".js\")\n","\n","def load_model_from_config(ckpt, verbose=False):\n","    print(f\"Loading model from {ckpt}\")\n","    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n","    if \"global_step\" in pl_sd:\n","        print(f\"Global Step: {pl_sd['global_step']}\")\n","    sd = pl_sd[\"state_dict\"]\n","    return sd\n","\n","def chunk(it, size):\n","    it = iter(it)\n","    return iter(lambda: tuple(islice(it, size)), ())\n","\n","\n","def load_img(image, h0, w0):\n","    image = image.convert(\"RGB\")\n","    w, h = image.size\n","    print(f\"loaded input image of size ({w}, {h})\")\n","    if h0 is not None and w0 is not None:\n","        h, w = h0, w0\n","\n","    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n","\n","    print(f\"New image size ({w}, {h})\")\n","    image = image.resize((w, h), resample=Image.LANCZOS)\n","    image = np.array(image).astype(np.float32) / 255.0\n","    image = image[None].transpose(0, 3, 1, 2)\n","    image = torch.from_numpy(image)\n","    return 2.0 * image - 1.0\n","\n","\n","def load_mask(mask, h0, w0, newH, newW, invert=False):\n","    image = mask.convert(\"RGB\")\n","    w, h = image.size\n","    print(f\"loaded input mask of size ({w}, {h})\")\n","    if h0 is not None and w0 is not None:\n","        h, w = h0, w0\n","\n","    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n","\n","    print(f\"New mask size ({w}, {h})\")\n","    image = image.resize((newW, newH), resample=Image.LANCZOS)\n","    # image = image.resize((64, 64), resample=Image.LANCZOS)\n","    image = np.array(image)\n","\n","    if invert:\n","        print(\"inverted\")\n","        where_0, where_1 = np.where(image == 0), np.where(image == 255)\n","        image[where_0], image[where_1] = 255, 0\n","    image = image.astype(np.float32) / 255.0\n","    image = image[None].transpose(0, 3, 1, 2)\n","    image = torch.from_numpy(image)\n","    return image\n","\n","\n","async def get_logs():\n","    return \"\\n\".join([x for x in open(\"log.txt\", \"r\", encoding=\"utf8\").readlines()] +\n","                     [y for y in open(\"tqdm.txt\", \"r\", encoding=\"utf8\").readlines()])\n","\n","\n","async def get_nvidia_smi():\n","    proc = await asyncio.create_subprocess_shell('nvidia-smi', stdout=asyncio.subprocess.PIPE)\n","    stdout, stderr = await proc.communicate()\n","    return str(stdout)\n","\n","def toImgOpenCV(imgPIL):  # Conver imgPIL to imgOpenCV\n","    i = np.array(imgPIL)  # After mapping from PIL to numpy : [R,G,B,A]\n","    # numpy Image Channel system: [B,G,R,A]\n","    red = i[:, :, 0].copy()\n","    i[:, :, 0] = i[:, :, 2].copy()\n","    i[:, :, 2] = red\n","    return i\n","\n","\n","def generate_img2img(\n","        image,\n","        negative_prompt,\n","        prompt,\n","        strength,\n","        ddim_steps,\n","        n_iter,\n","        batch_size,\n","        Width,\n","        Height,\n","        scale,\n","        ddim_eta,\n","        unet_bs,\n","        device,\n","        seed,\n","        outdir,\n","        img_format,\n","        turbo,\n","        full_precision,\n","        sampler,\n","        speed_mp,\n","):\n","    logging.info(f\"prompt: {prompt}, W: {Width}, H: {Height}\")\n","    try:\n","        init_image = load_img(image['image'], Height, Width).to(device)\n","    except:\n","        init_image = load_img(image, Height, Width).to(device)\n","    model.unet_bs = unet_bs\n","    model.turbo = turbo\n","    model.cdevice = device\n","    modelCS.cond_stage_model.device = device\n","\n","    try:\n","        seed = int(seed)\n","    except:\n","        seed = randint(0, 1000000)\n","\n","    if device != \"cpu\" and not full_precision:\n","        model.half()\n","        modelCS.half()\n","        modelFS.half()\n","        init_image = init_image.half()\n","\n","    tic = time.time()\n","    os.makedirs(outdir, exist_ok=True)\n","    outpath = outdir\n","    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt)))[:150]\n","    os.makedirs(sample_path, exist_ok=True)\n","    base_count = len(os.listdir(sample_path))\n","\n","    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n","    assert prompt is not None\n","    data = [batch_size * [prompt]]\n","\n","    modelFS.to(device)\n","\n","    init_image = repeat(init_image, \"1 ... -> b ...\", b=batch_size)\n","    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n","    try:\n","        image['mask']\n","        use_mask = True\n","    except:\n","        use_mask = False\n","    if use_mask:\n","        mask = load_mask(image['mask'], Height, Width, init_latent.shape[2], init_latent.shape[3], True).to(device)\n","        mask = mask[0][0].unsqueeze(0).repeat(4, 1, 1).unsqueeze(0)\n","        mask = repeat(mask, '1 ... -> b ...', b=batch_size)\n","        if device != \"cpu\" and not full_precision:\n","            mask = mask.half().to(device)\n","\n","    if device != \"cpu\":\n","        mem = torch.cuda.memory_allocated() / 1e6\n","        modelFS.to(\"cpu\")\n","        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","            time.sleep(1)\n","\n","    assert 0.0 <= strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n","    t_enc = int(strength * ddim_steps)\n","    print(f\"target t_enc is {t_enc} steps\")\n","\n","    if not full_precision and device != \"cpu\":\n","        precision_scope = autocast\n","    else:\n","        precision_scope = nullcontext\n","\n","    all_samples = []\n","    seeds = \"\"\n","    with torch.no_grad():\n","        for _ in trange(n_iter, desc=\"Sampling\"):\n","            for prompts in tqdm(data, desc=\"data\"):\n","                with precision_scope(\"cuda\"):\n","                    modelCS.to(device)\n","                    uc = None\n","                    if scale != 1.0:\n","                        uc = modelCS.get_learned_conditioning(\n","                            batch_size * [negative_prompt if negative_prompt is not None else \"\"])\n","                    if isinstance(prompts, tuple):\n","                        prompts = list(prompts)\n","\n","                    subprompts, weights = split_weighted_subprompts(prompts[0])\n","                    if len(subprompts) > 1:\n","                        c = torch.zeros_like(uc)\n","                        totalWeight = sum(weights)\n","                        # normalize each \"sub prompt\" and add it\n","                        for i in range(len(subprompts)):\n","                            weight = weights[i]\n","                            # if not skip_normalize:\n","                            weight = weight / totalWeight\n","                            c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n","                    else:\n","                        c = modelCS.get_learned_conditioning(prompts)\n","\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelCS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","\n","                    # encode (scaled latent)\n","                    z_enc = model.stochastic_encode(\n","                        init_latent, torch.tensor([t_enc] * batch_size).to(device), seed, ddim_eta, ddim_steps\n","                    )\n","                    # decode it\n","                    samples_ddim = model.sample(\n","                        t_enc,\n","                        c,\n","                        z_enc,\n","                        unconditional_guidance_scale=scale,\n","                        unconditional_conditioning=uc,\n","                        sampler=sampler,\n","                        speed_mp=speed_mp,\n","                        batch_size=batch_size,\n","                        x_T=init_latent if use_mask else None,\n","                        mask=mask if use_mask else None\n","                    )\n","\n","                    modelFS.to(device)\n","                    print(\"saving images\")\n","                    for i in range(batch_size):\n","                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n","                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                        all_samples.append(x_sample.to(\"cpu\"))\n","                        x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                        Image.fromarray(x_sample.astype(np.uint8)).save(\n","                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.{img_format}\")\n","                        )\n","                        seeds += str(seed) + \",\"\n","                        seed += 1\n","                        base_count += 1\n","\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelFS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","\n","                    del samples_ddim\n","                    del x_sample\n","                    del x_samples_ddim\n","                    print(\"memory_final = \", torch.cuda.memory_allocated() / 1e6)\n","\n","    toc = time.time()\n","\n","    time_taken = (toc - tic) / 60.0\n","    grid = torch.cat(all_samples, 0)\n","    grid = make_grid(grid, nrow=n_iter)\n","    grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n","\n","    txt = (\n","            \"Samples finished in \"\n","            + str(round(time_taken, 3))\n","            + \" minutes and exported to \\n\"\n","            + sample_path\n","            + \"\\nSeeds used = \"\n","            + seeds[:-1]\n","    )\n","    return Image.fromarray(grid.astype(np.uint8)), txt\n","\n","\n","def upscale2x(img):\n","    img = Image.fromarray(upsampler.enhance(img, outscale=2)[0])\n","    return img, f\"Upscaled to resolution: {img.size}\"\n","\n","\n","def face_restore(img):\n","    only_center_face = False\n","    draw_box = False\n","    codeformer_fidelity = 0.5\n","    upscale = 2\n","    face_upsample = True\n","    detection_model = \"retinaface_resnet50\"\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    face_helper = FaceRestoreHelper(\n","        True,\n","        face_size=512,\n","        crop_ratio=(1, 1),\n","        det_model=detection_model,\n","        save_ext=\"png\",\n","        use_parse=True,\n","        device=device,\n","    )\n","    codeformer_net.to(device)\n","    bg_upsampler = upsampler\n","    face_upsampler = upsampler\n","    face_helper.read_image(img)\n","    num_det_faces = face_helper.get_face_landmarks_5(\n","        only_center_face=only_center_face, resize=640, eye_dist_threshold=5\n","    )\n","    print(f\"\\tdetect {num_det_faces} faces\")\n","    # align and warp each face\n","    face_helper.align_warp_face()\n","\n","    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n","        # prepare data\n","        cropped_face_t = img2tensor(\n","            cropped_face / 255.0, bgr2rgb=True, float32=True\n","        )\n","        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n","        cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n","\n","        try:\n","            with torch.no_grad():\n","                output = codeformer_net(\n","                    cropped_face_t, w=codeformer_fidelity, adain=True\n","                )[0]\n","                restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n","            del output\n","            torch.cuda.empty_cache()\n","        except Exception as error:\n","            print(f\"\\tFailed inference for CodeFormer: {error}\")\n","            restored_face = tensor2img(\n","                cropped_face_t, rgb2bgr=True, min_max=(-1, 1)\n","            )\n","\n","        restored_face = restored_face.astype(\"uint8\")\n","        face_helper.add_restored_face(restored_face)\n","\n","    # paste_back\n","    # upsample the background\n","    if bg_upsampler is not None:\n","        # Now only support RealESRGAN for upsampling background\n","        bg_img = bg_upsampler.enhance(img, outscale=upscale)[0]\n","    else:\n","        bg_img = None\n","    face_helper.get_inverse_affine(None)\n","    # paste each restored face to the input image\n","    if face_upsample and face_upsampler is not None:\n","        restored_img = face_helper.paste_faces_to_input_image(\n","            upsample_img=bg_img,\n","            draw_box=draw_box,\n","            face_upsampler=face_upsampler,\n","        )\n","    else:\n","        restored_img = face_helper.paste_faces_to_input_image(\n","            upsample_img=bg_img, draw_box=draw_box\n","        )\n","    img = Image.fromarray(restored_img)\n","    return img, f\"Fixed a face, new img size: {img.size}\"\n","\n","\n","def generate_txt2img(\n","        prompt,\n","        negative_prompt,\n","        ddim_steps,\n","        n_iter,\n","        batch_size,\n","        Width,\n","        Height,\n","        scale,\n","        ddim_eta,\n","        unet_bs,\n","        device,\n","        seed,\n","        outdir,\n","        img_format,\n","        turbo,\n","        full_precision,\n","        sampler,\n","        speed_mp\n","):\n","    logging.info(f\"prompt: {prompt}, W: {Width}, H: {Height}\")\n","    C = 4\n","    f = 8\n","    start_code = None\n","    model.to(device)\n","    model.unet_bs = unet_bs\n","    model.turbo = turbo\n","    model.cdevice = device\n","    modelCS.cond_stage_model.device = device\n","\n","    if seed == \"\":\n","        seed = randint(0, 1000000)\n","    seed = int(seed)\n","    seed_everything(seed)\n","\n","    if device != \"cpu\" and not full_precision:\n","        model.half()\n","        modelFS.half()\n","        modelCS.half()\n","\n","    tic = time.time()\n","    os.makedirs(outdir, exist_ok=True)\n","    outpath = outdir\n","    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt.replace(\"/\", \"\"))))[:150]\n","    os.makedirs(sample_path, exist_ok=True)\n","    base_count = len(os.listdir(sample_path))\n","\n","    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n","    assert prompt is not None\n","    data = [batch_size * [prompt]]\n","\n","    if device != \"cpu\" and not full_precision:\n","        precision_scope = autocast\n","    else:\n","        precision_scope = nullcontext\n","\n","    seeds = \"\"\n","    with torch.no_grad():\n","        all_samples = list()\n","        for _ in trange(n_iter, desc=\"Sampling\"):\n","            for prompts in tqdm(data, desc=\"data\"):\n","                with precision_scope(\"cuda\"):\n","                    modelCS.to(device)\n","                    uc = None\n","                    if scale != 1.0:\n","                        uc = modelCS.get_learned_conditioning(\n","                            batch_size * [negative_prompt if negative_prompt is not None else \"\"])\n","                    if isinstance(prompts, tuple):\n","                        prompts = list(prompts)\n","\n","                    subprompts, weights = split_weighted_subprompts(prompts[0])\n","                    if len(subprompts) > 1:\n","                        c = torch.zeros_like(uc)\n","                        totalWeight = sum(weights)\n","                        # normalize each \"sub prompt\" and add it\n","                        for i in range(len(subprompts)):\n","                            weight = weights[i]\n","                            # if not skip_normalize:\n","                            weight = weight / totalWeight\n","                            c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n","                    else:\n","                        c = modelCS.get_learned_conditioning(prompts)\n","\n","                    shape = [batch_size, C, Height // f, Width // f]\n","\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelCS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","                    samples_ddim = model.sample(\n","                        S=ddim_steps,\n","                        conditioning=c,\n","                        seed=seed,\n","                        shape=shape,\n","                        verbose=False,\n","                        unconditional_guidance_scale=scale,\n","                        unconditional_conditioning=uc,\n","                        eta=ddim_eta,\n","                        x_T=start_code,\n","                        sampler=sampler,\n","                        speed_mp=speed_mp\n","                    )\n","\n","                    modelFS.to(device)\n","                    model.cpu()\n","                    logging.info(\"saving images\")\n","                    for i in range(batch_size):\n","                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n","                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                        all_samples.append(x_sample.to(\"cpu\"))\n","                        x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                        Image.fromarray(x_sample.astype(np.uint8)).save(\n","                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.{img_format}\")\n","                        )\n","                        seeds += str(seed) + \",\"\n","                        seed += 1\n","                        base_count += 1\n","\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelFS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","\n","                    del samples_ddim\n","                    del x_sample\n","                    del x_samples_ddim\n","                    logging.info(str(\"memory_final = \" + str(torch.cuda.memory_allocated() / 1e6)))\n","\n","    toc = time.time()\n","\n","    time_taken = (toc - tic) / 60.0\n","    grid = torch.cat(all_samples, 0)\n","    grid = make_grid(grid, nrow=n_iter)\n","    grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n","    txt = (\n","            \"Samples finished in \"\n","            + str(round(time_taken, 3))\n","            + \" minutes and exported to \"\n","            + sample_path\n","            + \"\\nSeeds used = \"\n","            + seeds[:-1]\n","    )\n","    return Image.fromarray(grid.astype(np.uint8)), txt\n","\n","\n","def generate_img2img_interp(\n","        image,\n","        prompt,\n","        strength,\n","        ddim_steps,\n","        n_iter,\n","        batch_size,\n","        Width,\n","        Height,\n","        scale,\n","        ddim_eta,\n","        unet_bs,\n","        device,\n","        seed,\n","        outdir,\n","        img_format,\n","        turbo,\n","        full_precision,\n","        sampler,\n","        speed_mp,\n","        n_interpolate_samples\n","):\n","    logging.info(f\"prompt: {prompt}, W: {Width}, H: {Height}\")\n","    try:\n","        init_image = load_img(image['image'], Height, Width).to(device)\n","    except:\n","        init_image = load_img(image, Height, Width).to(device)\n","    model.unet_bs = unet_bs\n","    model.turbo = turbo\n","    model.cdevice = device\n","    modelCS.cond_stage_model.device = device\n","\n","    try:\n","        seed = int(seed)\n","    except:\n","        seed = randint(0, 1000000)\n","\n","    if device != \"cpu\" and not full_precision:\n","        model.half()\n","        modelCS.half()\n","        modelFS.half()\n","        init_image = init_image.half()\n","\n","    os.makedirs(outdir, exist_ok=True)\n","    outpath = outdir\n","    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt)))[:150]\n","    os.makedirs(sample_path, exist_ok=True)\n","    base_count = len(os.listdir(sample_path))\n","\n","    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n","    assert prompt is not None\n","    data = [batch_size * [prompt]]\n","\n","    modelFS.to(device)\n","\n","    init_image = repeat(init_image, \"1 ... -> b ...\", b=batch_size)\n","    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n","    if device != \"cpu\":\n","        mem = torch.cuda.memory_allocated() / 1e6\n","        modelFS.to(\"cpu\")\n","        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","            time.sleep(1)\n","\n","    assert 0.0 <= strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n","    t_enc = int(strength * ddim_steps)\n","    print(f\"target t_enc is {t_enc} steps\")\n","\n","    if not full_precision and device != \"cpu\":\n","        precision_scope = autocast\n","    else:\n","        precision_scope = nullcontext\n","\n","    seeds = \"\"\n","    with torch.no_grad():\n","        for _ in trange(n_iter, desc=\"Sampling\"):\n","            for prompts in tqdm(data, desc=\"data\"):\n","                with precision_scope(\"cuda\"):\n","                    modelCS.to(device)\n","                    uc = None\n","                    if scale != 1.0:\n","                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n","                    if isinstance(prompts, tuple):\n","                        prompts = list(prompts)\n","\n","                    subprompts, weights = split_weighted_subprompts(prompts[0])\n","                    if len(subprompts) > 1:\n","                        c = torch.zeros_like(uc)\n","                        totalWeight = sum(weights)\n","                        # normalize each \"sub prompt\" and add it\n","                        for i in range(len(subprompts)):\n","                            weight = weights[i]\n","                            # if not skip_normalize:\n","                            weight = weight / totalWeight\n","                            c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n","                    else:\n","                        c = modelCS.get_learned_conditioning(prompts)\n","\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelCS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","\n","                    # encode (scaled latent)\n","                    true_z_enc = model.stochastic_encode(\n","                        init_latent, torch.tensor([t_enc] * batch_size).to(device), seed, ddim_eta, ddim_steps\n","                    )\n","                    # decode it\n","                    samples_ddim = model.sample(\n","                        t_enc,\n","                        c,\n","                        true_z_enc,\n","                        unconditional_guidance_scale=scale,\n","                        unconditional_conditioning=uc,\n","                        sampler=sampler,\n","                        speed_mp=speed_mp\n","                    )\n","                    modelFS.to(device)\n","                    print(\"saving frames\")\n","                    all_time_samples = []\n","                    for ij in range(n_interpolate_samples):\n","                        temp_all_samples = []\n","                        for i in range(batch_size):\n","                            start0_sample = samples_ddim[i].unsqueeze(0)\n","                            interp_sample = torch.lerp(init_latent, start0_sample, (ij / n_interpolate_samples))\n","                            x_samples_ddim = modelFS.decode_first_stage(interp_sample)\n","                            x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                            temp_all_samples.append(x_sample.to(\"cpu\"))\n","                            x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                            Image.fromarray(x_sample.astype(np.uint8)).save(\n","                                os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.{img_format}\")\n","                            )\n","                            seeds += str(seed) + \",\"\n","                            base_count += 1\n","                        grid = torch.cat(temp_all_samples, 0)\n","                        grid = make_grid(grid, nrow=n_iter)\n","                        grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n","                        all_time_samples.append(Image.fromarray(grid.astype(np.uint8)))\n","                    if device != \"cpu\":\n","                        mem = torch.cuda.memory_allocated() / 1e6\n","                        modelFS.to(\"cpu\")\n","                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                            time.sleep(1)\n","\n","                    del samples_ddim\n","                    del x_sample\n","                    del x_samples_ddim\n","                    print(\"memory_final = \", torch.cuda.memory_allocated() / 1e6)\n","        # all_samples.append(all_time_samples)\n","    all_time_samples = [toImgOpenCV(img) for img in all_time_samples]\n","    out = cv2.VideoWriter(\"tempfile.mp4\", cv2.VideoWriter_fourcc(*'h264'), 15, (all_time_samples[0].shape[1], all_time_samples[0].shape[0]))\n","    for img in all_time_samples:\n","        out.write(img)\n","    out.release()\n","\n","    return \"tempfile.mp4\", f\"yeah here's your video {Width}x{Height}\"\n","\n","\n","def generate_double_triple(\n","        prompt,\n","        ddim_steps,\n","        img2img_strength,\n","        Width,\n","        Height,\n","        scale,\n","        ddim_eta,\n","        unet_bs,\n","        device,\n","        seed,\n","        outdir,\n","        img_format,\n","        turbo,\n","        full_precision,\n","        sampler,\n","        speed_mp,\n","        upscale_reso\n","):\n","    C = 4\n","    f = 8\n","    start_code = None\n","    model.unet_bs = unet_bs\n","    model.turbo = turbo\n","    model.cdevice = device\n","    modelCS.cond_stage_model.device = device\n","\n","    if seed == \"\":\n","        seed = randint(0, 1000000)\n","    seed = int(seed)\n","    seed_everything(seed)\n","\n","    if device != \"cpu\" and not full_precision:\n","        model.half()\n","        modelFS.half()\n","        modelCS.half()\n","\n","    tic = time.time()\n","    os.makedirs(outdir, exist_ok=True)\n","    outpath = outdir\n","    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt.replace(\"/\", \"\"))))[:150]\n","    os.makedirs(sample_path, exist_ok=True)\n","    base_count = len(os.listdir(sample_path))\n","\n","    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n","    assert prompt is not None\n","    data = [1 * [prompt]]\n","\n","    if device != \"cpu\" and not full_precision:\n","        precision_scope = autocast\n","    else:\n","        precision_scope = nullcontext\n","\n","    seeds = \"\"\n","    with torch.no_grad():\n","        all_samples = list()\n","        for prompts in tqdm(data, desc=\"data\"):\n","            with precision_scope(\"cuda\"):\n","                modelCS.to(device)\n","                uc = None\n","                if scale != 1.0:\n","                    uc = modelCS.get_learned_conditioning(1 * [\"\"])\n","                if isinstance(prompts, tuple):\n","                    prompts = list(prompts)\n","\n","                subprompts, weights = split_weighted_subprompts(prompts[0])\n","                if len(subprompts) > 1:\n","                    c = torch.zeros_like(uc)\n","                    totalWeight = sum(weights)\n","                    # normalize each \"sub prompt\" and add it\n","                    for i in range(len(subprompts)):\n","                        weight = weights[i]\n","                        # if not skip_normalize:\n","                        weight = weight / totalWeight\n","                        c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n","                else:\n","                    c = modelCS.get_learned_conditioning(prompts)\n","\n","                shape = [1, C, Height // f, Width // f]\n","\n","                if device != \"cpu\":\n","                    mem = torch.cuda.memory_allocated() / 1e6\n","                    modelCS.to(\"cpu\")\n","                    while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                        time.sleep(1)\n","\n","                samples_ddim = model.sample(\n","                    S=ddim_steps,\n","                    conditioning=c,\n","                    seed=seed,\n","                    shape=shape,\n","                    verbose=False,\n","                    unconditional_guidance_scale=scale,\n","                    unconditional_conditioning=uc,\n","                    eta=ddim_eta,\n","                    x_T=start_code,\n","                    sampler=sampler,\n","                    speed_mp=speed_mp\n","                )\n","\n","                modelFS.to(device)\n","\n","                x_samples_ddim = modelFS.decode_first_stage(samples_ddim[0].unsqueeze(0))\n","                x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                Image.fromarray(x_sample.astype(np.uint8)).save(\n","                    os.path.join(sample_path, \"seed_\" + str(seed) + \"_step1_\" + f\"{base_count:05}.{img_format}\")\n","                )\n","\n","                ### STEP 2\n","\n","                init_image = repeat(\n","                    load_img(Image.fromarray(x_sample.astype(np.uint8)), Height * 2, Width * 2).to(device),\n","                    \"1 ... -> b ...\", b=1)\n","                init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))\n","\n","                modelFS.cpu()\n","                model.to(device)\n","\n","                z_enc = model.stochastic_encode(\n","                    init_latent, torch.tensor([int(img2img_strength * ddim_steps)]).to(device), seed, ddim_eta,\n","                    ddim_steps\n","                ).to(device)\n","\n","                samples_ddim = model.sample(\n","                    int(img2img_strength * ddim_steps // 2),\n","                    c,\n","                    z_enc,\n","                    unconditional_guidance_scale=scale,\n","                    unconditional_conditioning=uc,\n","                    sampler=\"ddim\",\n","                    speed_mp=speed_mp\n","                )\n","\n","                modelFS.to(device)\n","                model.cpu()\n","                modelCS.to(\"cpu\")\n","                torch.cuda.empty_cache()\n","\n","                x_samples_ddim = modelFS.decode_first_stage(samples_ddim[0].unsqueeze(0))\n","                x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                if upscale_reso < 3:\n","                    all_samples.append(x_sample.cpu())\n","                x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                Image.fromarray(x_sample.astype(np.uint8)).save(\n","                    os.path.join(sample_path, \"seed_\" + str(seed) + \"_step2_\" + f\"{base_count:05}.{img_format}\")\n","                )\n","\n","                ### STEP 3\n","                if upscale_reso >= 3:\n","                    init_image = repeat(\n","                        load_img(Image.fromarray(x_sample.astype(np.uint8)), Height * 3, Width * 3).to(device),\n","                        \"1 ... -> b ...\", b=1)\n","                    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))\n","\n","                    modelFS.cpu()\n","                    model.to(device)\n","\n","                    z_enc = model.stochastic_encode(\n","                        init_latent, torch.tensor([int(img2img_strength * ddim_steps)]).to(device), seed, ddim_eta,\n","                        ddim_steps\n","                    ).to(device)\n","\n","                    samples_ddim = model.sample(\n","                        int(img2img_strength * ddim_steps // 2),\n","                        c,\n","                        z_enc,\n","                        unconditional_guidance_scale=scale,\n","                        unconditional_conditioning=uc,\n","                        sampler=\"ddim\",\n","                        speed_mp=speed_mp\n","                    )\n","\n","                    print(\"saving images\")\n","                    model.cpu()\n","                    modelFS.to(device)\n","\n","                    x_samples_ddim = modelFS.decode_first_stage(samples_ddim[0].unsqueeze(0))\n","                    x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n","                    all_samples.append(x_sample.to(\"cpu\"))\n","                    x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n","                    Image.fromarray(x_sample.astype(np.uint8)).save(\n","                        os.path.join(sample_path, \"seed_\" + str(seed) + \"_step3_\" + f\"{base_count:05}.{img_format}\")\n","                    )\n","\n","                if device != \"cpu\":\n","                    mem = torch.cuda.memory_allocated() / 1e6\n","                    modelFS.to(\"cpu\")\n","                    while torch.cuda.memory_allocated() / 1e6 >= mem:\n","                        time.sleep(1)\n","\n","                del samples_ddim\n","                del x_sample\n","                del x_samples_ddim\n","                print(\"memory_final = \", torch.cuda.memory_allocated() / 1e6)\n","\n","    toc = time.time()\n","\n","    time_taken = (toc - tic) / 60.0\n","    grid = torch.cat(all_samples, 0)\n","    grid = make_grid(grid, nrow=1)\n","    grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n","\n","    txt = (\n","            \"Samples finished in \"\n","            + str(round(time_taken, 3))\n","            + \" minutes and exported to \"\n","            + sample_path\n","            + \"\\nSeeds used = \"\n","            + seeds[:-1]\n","    )\n","    return Image.fromarray(grid.astype(np.uint8)), txt\n","\n","\n","def download_codeformer():\n","    pretrain_model_url = {\n","        'codeformer': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n","        'detection': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/detection_Resnet50_Final.pth',\n","        'parsing': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/parsing_parsenet.pth',\n","        'realesrgan': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth'\n","    }\n","    # download weights\n","    if not os.path.exists(\"models/codeformer/\" + \"CodeFormer/codeformer.pth\"):\n","        load_file_from_url(url=pretrain_model_url['codeformer'], model_dir=\"models/codeformer/\" + \"CodeFormer\",\n","                           progress=True, file_name=None)\n","    if not os.path.exists(\"models/codeformer/\" + \"facelib/detection_Resnet50_Final.pth\"):\n","        load_file_from_url(url=pretrain_model_url['detection'], model_dir=\"models/codeformer/\" + \"facelib\",\n","                           progress=True,\n","                           file_name=None)\n","    if not os.path.exists(\"models/codeformer/\" + \"facelib/parsing_parsenet.pth\"):\n","        load_file_from_url(url=pretrain_model_url['parsing'], model_dir=\"models/codeformer/\" + \"facelib\", progress=True,\n","                           file_name=None)\n","    if not os.path.exists(\"models/codeformer/\" + \"realesrgan/RealESRGAN_x2plus.pth\"):\n","        load_file_from_url(url=pretrain_model_url['realesrgan'], model_dir=\"models/codeformer/\" + \"realesrgan\",\n","                           progress=True, file_name=None)\n","\n","\n","# set enhancer with RealESRGAN\n","def set_realesrgan():\n","    half = True if torch.cuda.is_available() else False\n","    model = RRDBNet(\n","        num_in_ch=3,\n","        num_out_ch=3,\n","        num_feat=64,\n","        num_block=23,\n","        num_grow_ch=32,\n","        scale=2,\n","    )\n","    upsampler = RealESRGANer(\n","        scale=2,\n","        model_path=\"models/codeformer/\" + \"realesrgan/RealESRGAN_x2plus.pth\",\n","        model=model,\n","        tile=400,\n","        tile_pad=40,\n","        pre_pad=0,\n","        half=half,\n","    )\n","    return upsampler\n","\n","\n","class TqdmLoggingHandler(logging.Handler):\n","    def __init__(self, level=logging.NOTSET):\n","        super().__init__(level)\n","\n","    def emit(self, record):\n","        try:\n","            msg = self.format(record)\n","            tqdm.write(msg)\n","            self.flush()\n","        except Exception:\n","            self.handleError(record)\n","\n","print(\"Downloading codeformer weights..\")\n","download_codeformer()\n","\n","print(\"Loading realesr..\")\n","upsampler = set_realesrgan()\n","print(\"Loading codeformer..\")\n","codeformer_net = ARCH_REGISTRY.get(\"CodeFormer\")(\n","  dim_embd=512,\n","  codebook_size=1024,\n","  n_head=8,\n","  n_layers=9,\n","  connect_list=[\"32\", \"64\", \"128\", \"256\"],\n",")\n","checkpoint = torch.load(\"models/codeformer/\" + \"CodeFormer/codeformer.pth\")[\"params_ema\"]\n","codeformer_net.load_state_dict(checkpoint)\n","codeformer_net.eval().cuda()\n","gc.collect()\n","\n","\n","config = \"optimizedSD/v1-inference.yaml\"\n","ckpt = '/content/drive/MyDrive/sd-v1-4.ckpt' #@param {type:\"string\"}\n","sd = load_model_from_config(f\"{ckpt}\")\n","li, lo = [], []\n","for key, v_ in sd.items():\n","    sp = key.split(\".\")\n","    if (sp[0]) == \"model\":\n","        if \"input_blocks\" in sp:\n","            li.append(key)\n","        elif \"middle_block\" in sp:\n","            li.append(key)\n","        elif \"time_embed\" in sp:\n","            li.append(key)\n","        else:\n","            lo.append(key)\n","for key in li:\n","    sd[\"model1.\" + key[6:]] = sd.pop(key)\n","for key in lo:\n","    sd[\"model2.\" + key[6:]] = sd.pop(key)\n","\n","config = OmegaConf.load(f\"{config}\")\n","\n","model = instantiate_from_config(config.modelUNet)\n","_, _ = model.load_state_dict(sd, strict=False)\n","model.eval()\n","\n","modelCS = instantiate_from_config(config.modelCondStage)\n","_, _ = modelCS.load_state_dict(sd, strict=False)\n","modelCS.eval()\n","\n","modelFS = instantiate_from_config(config.modelFirstStage)\n","_, _ = modelFS.load_state_dict(sd, strict=False)\n","modelFS.eval()\n","del sd\n","gc.collect()"],"metadata":{"cellView":"form","id":"K_MrXDzIYLNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Okay next run the ultimate interface"],"metadata":{"id":"NyafrRNTF1Xe"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["#@title Ultimate Gradio Interface\n","import gradio as gr\n","demo = gr.Blocks()\n","with demo:\n","    with gr.Tab(\"txt2img\"):\n","        with gr.Column():\n","            gr.Markdown(\"# Generate images from text (neonsecret's adjustments)\")\n","            with gr.Row():\n","                with gr.Column():\n","                    out_image = gr.Image(label=\"Output Image\")\n","                    gen_res = gr.Text(label=\"Generation results\")\n","                    b1 = gr.Button(\"Generate!\")\n","                    b4 = gr.Button(\"Face correction\")\n","                    b5 = gr.Button(\"Upscale 2x\")\n","                with gr.Column():\n","                    with gr.Box():\n","                        b4.click(face_restore, inputs=[out_image], outputs=[out_image, gen_res])\n","                        b5.click(upscale2x, inputs=[out_image], outputs=[out_image, gen_res])\n","                        b1.click(generate_txt2img, inputs=[\n","                            gr.Text(label=\"Your Prompt\"),\n","                            gr.Text(label=\"Your Negative Prompt\"),\n","                            gr.Slider(1, 200, value=50, label=\"Sampling Steps\"),\n","                            gr.Slider(1, 100, step=1, label=\"Number of images\"),\n","                            gr.Slider(1, 100, step=1, label=\"Batch size\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n","                            gr.Slider(0, 50, value=7.5, step=0.1, label=\"Guidance scale\"),\n","                            gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n","                            gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n","                            gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n","                            gr.Text(label=\"Seed\"),\n","                            gr.Text(value=output_path, label=\"Outputs path\"),\n","                            gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n","                            gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n","                            gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n","                            gr.Radio(\n","                                [\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"],\n","                                value=\"plms\", label=\"Sampler\"),\n","                            gr.Checkbox(value=True, label=\"Lightning Attention (only on linux + xformers installed)\"),\n","                        ], outputs=[out_image, gen_res])\n","    with gr.Tab(\"img2img\"):\n","        with gr.Column():\n","            gr.Markdown(\"# Generate images from images (neonsecret's adjustments)\")\n","            with gr.Row():\n","                with gr.Column():\n","                    out_image2 = gr.Image(label=\"Output Image\")\n","                    gen_res2 = gr.Text(label=\"Generation results\")\n","                    b1 = gr.Button(\"Generate!\")\n","                    b4 = gr.Button(\"Face correction\")\n","                    b5 = gr.Button(\"Upscale 2x\")\n","                with gr.Column():\n","                    with gr.Box():\n","                        b4.click(face_restore, inputs=[out_image2], outputs=[out_image2, gen_res2])\n","                        b5.click(upscale2x, inputs=[out_image2], outputs=[out_image2, gen_res2])\n","                        b1.click(generate_img2img, inputs=[\n","                            gr.Image(tool=\"editor\", type=\"pil\", label=\"Initial image\"),\n","                            gr.Text(label=\"Your Prompt\"),\n","                            gr.Text(label=\"Your Negative Prompt\"),\n","                            gr.Slider(0, 1, value=0.75, label=\"Generated image strength\"),\n","                            gr.Slider(1, 200, value=50, label=\"Sampling Steps\"),\n","                            gr.Slider(1, 100, step=1, label=\"Number of images\"),\n","                            gr.Slider(1, 100, step=1, label=\"Batch size\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n","                            gr.Slider(0, 50, value=7.5, step=0.1, label=\"Guidance scale\"),\n","                            gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n","                            gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n","                            gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n","                            gr.Text(label=\"Seed\"),\n","                            gr.Text(value=output_path, label=\"Outputs path\"),\n","                            gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n","                            gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n","                            gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n","                            gr.Radio(\n","                                [\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"],\n","                                value=\"ddim\", label=\"Sampler\"),\n","                            gr.Checkbox(value=True, label=\"Lightning Attention (only on linux + xformers installed)\"),\n","                        ], outputs=[out_image2, gen_res2])\n","    with gr.Tab(\"img2img inpaint\"):\n","        with gr.Column():\n","            gr.Markdown(\"# Generate images from images (with a mask) (neonsecret's adjustments)\")\n","            with gr.Row():\n","                with gr.Column():\n","                    out_image3 = gr.Image(label=\"Output Image\")\n","                    gen_res3 = gr.Text(label=\"Generation results\")\n","                    b1 = gr.Button(\"Generate!\")\n","                    b4 = gr.Button(\"Face correction\")\n","                    b5 = gr.Button(\"Upscale 2x\")\n","                with gr.Column():\n","                    with gr.Box():\n","                        b4.click(face_restore, inputs=[out_image3], outputs=[out_image3, gen_res3])\n","                        b5.click(upscale2x, inputs=[out_image3], outputs=[out_image3, gen_res3])\n","                        b1.click(generate_img2img, inputs=[\n","                            gr.Image(tool=\"sketch\", type=\"pil\", label=\"Initial image with a mask\"),\n","                            gr.Text(label=\"Your Prompt\"),\n","                            gr.Text(label=\"Your Negative Prompt\"),\n","                            gr.Slider(0, 1, value=0.75, label=\"Generated image strength\"),\n","                            gr.Slider(1, 200, value=50, label=\"Sampling Steps\"),\n","                            gr.Slider(1, 100, step=1, label=\"Number of images\"),\n","                            gr.Slider(1, 100, step=1, label=\"Batch size\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n","                            gr.Slider(0, 50, value=7.5, step=0.1, label=\"Guidance scale\"),\n","                            gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n","                            gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n","                            gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n","                            gr.Text(label=\"Seed\"),\n","                            gr.Text(value=output_path, label=\"Outputs path\"),\n","                            gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n","                            gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n","                            gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n","                            gr.Radio(\n","                                [\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"],\n","                                value=\"ddim\", label=\"Sampler\"),\n","                            gr.Checkbox(value=True, label=\"Lightning Attention (only on linux + xformers installed)\"),\n","                        ], outputs=[out_image3, gen_res3])\n","    with gr.Tab(\"img2img interpolate\"):\n","        with gr.Column():\n","            gr.Markdown(\"# Generate a video interpolation from images\")\n","            gr.Markdown(\"### Press 'generation status' button to get the model output logs\")\n","            with gr.Row():\n","                with gr.Column():\n","                    out_video = gr.Video()\n","                    gen_res4 = gr.Text(label=\"Generation results\")\n","                    outs2 = [gr.Text(label=\"Logs\")]\n","                    outs3 = [gr.Text(label=\"nvidia-smi\")]\n","                    b1 = gr.Button(\"Generate!\")\n","                    b2 = gr.Button(\"generation status\")\n","                    b3 = gr.Button(\"nvidia-smi\")\n","                with gr.Column():\n","                    with gr.Box():\n","                        b1.click(generate_img2img_interp, inputs=[\n","                            gr.Image(tool=\"editor\", type=\"pil\", label=\"Initial image\"),\n","                            gr.Text(label=\"Your Prompt\"),\n","                            gr.Slider(0, 1, value=0.75, label=\"Generated image strength\"),\n","                            gr.Slider(1, 200, value=50, label=\"Sampling Steps\"),\n","                            gr.Slider(1, 100, step=1, label=\"Number of images\"),\n","                            gr.Slider(1, 100, step=1, label=\"Batch size\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n","                            gr.Slider(-25, 25, value=7.5, step=0.1, label=\"Guidance scale\"),\n","                            gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n","                            gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n","                            gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n","                            gr.Text(label=\"Seed\"),\n","                            gr.Text(value=output_path, label=\"Outputs path\"),\n","                            gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n","                            gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n","                            gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n","                            gr.Radio(\n","                                [\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"],\n","                                value=\"ddim\", label=\"Sampler\"),\n","                            gr.Checkbox(value=True, label=\"Lightning Attention (only on linux + xformers installed)\"),\n","                            gr.Slider(1, 120, value=60, step=1, label=\"How smooth the video will be\"),\n","                        ], outputs=[out_video, gen_res4])\n","                        b2.click(get_logs, inputs=[], outputs=outs2)\n","                        b3.click(get_nvidia_smi, inputs=[], outputs=outs3)\n","    with gr.Tab(\"txt2img 2x-3x upscale\"):\n","        with gr.Column():\n","            gr.Markdown(\"# Generate images from text using SD upscaling\")\n","            gr.Markdown(\"### Generate images in 2(3) steps - Wx -> 2Wx2H (-> 3Wx3H)\")\n","            gr.Markdown(\"### Press 'generation status' button to get the model output logs\")\n","            with gr.Row():\n","                with gr.Column():\n","                    out_image = gr.Image(label=\"Output Image\")\n","                    gen_res = gr.Text(label=\"Generation results\")\n","                    outs2 = [gr.Text(label=\"Logs\")]\n","                    outs3 = gr.Text(label=\"nvidia-smi\")\n","                    b1 = gr.Button(\"Generate!\")\n","                    b4 = gr.Button(\"Face correction\")\n","                    b5 = gr.Button(\"Upscale 2x\")\n","                    b2 = gr.Button(\"generation status\")\n","                    b3 = gr.Button(\"nvidia-smi\")\n","                with gr.Column():\n","                    with gr.Box():\n","                        b4.click(face_restore, inputs=[out_image], outputs=[out_image, gen_res])\n","                        b5.click(upscale2x, inputs=[out_image], outputs=[out_image, gen_res])\n","                        b1.click(generate_double_triple, inputs=[\n","                            gr.Text(label=\"Your Prompt\"),\n","                            gr.Slider(1, 200, value=50, label=\"Sampling Steps\"),\n","                            gr.Slider(0, 1, value=0.35, label=\"Upscaled image changes strength\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n","                            gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n","                            gr.Slider(-25, 25, value=7.5, step=0.1, label=\"Guidance scale\"),\n","                            gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n","                            gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n","                            gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n","                            gr.Text(label=\"Seed\"),\n","                            gr.Text(value=output_path, label=\"Outputs path\"),\n","                            gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n","                            gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n","                            gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n","                            gr.Radio(\n","                                [\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"],\n","                                value=\"plms\", label=\"Sampler\"),\n","                            gr.Checkbox(value=True, label=\"Lightning Attention (only on linux + xformers installed)\"),\n","                            gr.Slider(2, 3, value=2, step=1,\n","                                      label=\"Neural scaling factor, 3 will take much longer\"),\n","                        ], outputs=[out_image, gen_res])\n","                        b2.click(get_logs, inputs=[], outputs=outs2)\n","                        b3.click(get_nvidia_smi, inputs=[], outputs=[outs3])\n","\n","debug = False #@param {type:\"boolean\"}\n","demo.launch(share=True, debug=debug)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"gnYhjq2Gc1zc","cellView":"form"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[{"file_id":"https://github.com/neonsecret/stable-diffusion/blob/main/optimized_colab.ipynb","timestamp":1686011931959}]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}